{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sO-WkVPQjqUv"
      },
      "source": [
        "##EECS 4412 Project\n",
        "\n",
        "Author: Jason Lau\n",
        "\n",
        "Student#: 218835066"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7UBoZUlYn-sn",
        "outputId": "8a138ef4-c641-410a-fdd9-2afdeda54f90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "train_csv_path = '/content/drive/MyDrive/train_yelp_60k.csv'\n",
        "test_csv_path = '/content/drive/MyDrive/test_yelp_60k.csv'\n",
        "\n",
        "pre_test_df = pd.read_csv(test_csv_path)\n",
        "\n",
        "ids = pre_test_df['ID'] #Keep ids\n",
        "\n",
        "train_df = pd.read_csv(train_csv_path).drop(columns=[\"ID\"]) #Remove the ID column\n",
        "test_df = pre_test_df.drop(columns=[\"ID\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cKL0QnppPRQp",
        "outputId": "723ad77f-5a58-4072-fb63-d7e4e32bd2d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "import string\n",
        "\n",
        "#NLTK data\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "#Preprocessing\n",
        "def preprocess_text(text):\n",
        "  #Lowercase the text\n",
        "  text = text.lower()\n",
        "\n",
        "  #Tokenization\n",
        "  tokens = word_tokenize(text)\n",
        "\n",
        "  #Remove punctuation\n",
        "  tokens = [word for word in tokens if word not in string.punctuation]\n",
        "\n",
        "  #Remove stopwords\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "  #Recombine tokens into a string\n",
        "  return \" \".join(tokens)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bZ3UV6XJjq6X"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from xgboost import XGBClassifier\n",
        "import pandas as pd\n",
        "\n",
        "#classifier\n",
        "clf = XGBClassifier(n_estimators=500, learning_rate=0.1, max_depth=5)\n",
        "\n",
        "X = train_df[\"Text\"].apply(preprocess_text)  #Features, preprocessing\n",
        "y = train_df[\"Class\"]  #Target\n",
        "\n",
        "#Convert string labels to numeric values\n",
        "le = LabelEncoder()\n",
        "y_encoded = le.fit_transform(y)\n",
        "\n",
        "#Split data into train and test sets\n",
        "X_train, X_test, y_train_encoded, y_test_encoded = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "#Keep the original string labels for reporting\n",
        "_, _, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "#BOW approach using TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X_train_bow = vectorizer.fit_transform(X_train)\n",
        "X_test_bow = vectorizer.transform(X_test)\n",
        "\n",
        "#Applying SMOTE for class balancing\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_bow_smote, y_train_encoded_smote = smote.fit_resample(X_train_bow, y_train_encoded)\n",
        "\n",
        "#Feature selection using SelectKBest and chi2\n",
        "k = 1500  #Number of features to select\n",
        "select_k_best = SelectKBest(chi2, k=k)\n",
        "\n",
        "#Apply feature selection to the training and test sets\n",
        "X_train_bow_selected = select_k_best.fit_transform(X_train_bow_smote, y_train_encoded_smote)\n",
        "X_test_bow_selected = select_k_best.transform(X_test_bow)\n",
        "\n",
        "#XGBoost Evaluation\n",
        "print(\"Evaluating XGBoost:\")\n",
        "\n",
        "#BOW Evaluation\n",
        "clf.fit(X_train_bow_selected, y_train_encoded_smote)\n",
        "y_pred_encoded = clf.predict(X_test_bow_selected)\n",
        "\n",
        "#Convert numeric predictions back to original labels for evaluation\n",
        "y_pred_bow = le.inverse_transform(y_pred_encoded)\n",
        "\n",
        "bow_accuracy = accuracy_score(y_test, y_pred_bow)\n",
        "bow_precision = precision_score(y_test, y_pred_bow, average='macro')\n",
        "bow_recall = recall_score(y_test, y_pred_bow, average='macro')\n",
        "bow_f1 = f1_score(y_test, y_pred_bow, average='macro')\n",
        "\n",
        "#Show Results\n",
        "print(f\"BOW Classification Report For XGBoost\", classification_report(y_test, y_pred_bow))\n",
        "\n",
        "\n",
        "\n",
        "#Predictions using XGBoost\n",
        "test_texts = test_df[\"Text\"].apply(preprocess_text) #Preprocess the test data\n",
        "\n",
        "#Transform using the same vectorizer and feature selector used during training\n",
        "X_final_test_bow = vectorizer.transform(test_texts)\n",
        "X_final_test_selected = select_k_best.transform(X_final_test_bow)\n",
        "\n",
        "#Predictions using the trained classifier\n",
        "final_preds_encoded = clf.predict(X_final_test_selected)\n",
        "\n",
        "#Convert predictions back to original labels\n",
        "final_preds = le.inverse_transform(final_preds_encoded)\n",
        "\n",
        "#Create DataFrame\n",
        "submission_df = pd.DataFrame({\n",
        "    \"ID\": ids,\n",
        "    \"Class\": final_preds\n",
        "})\n",
        "\n",
        "prediction_counts = pd.Series(final_preds).value_counts()\n",
        "print(\"XGBoost Predictions:\")\n",
        "print(prediction_counts)\n",
        "\n",
        "# Save predictions to CSV\n",
        "submission_df.to_csv(\"/content/drive/MyDrive/yelp_predictions.csv\", index=False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}